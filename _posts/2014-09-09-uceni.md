---
layout: post
title: Statistické metody a strojové učení v počítačové lingvistice
---

Generativní a diskriminativní modely
---
- diskriminativní modely modelují rovnou
    <math>
    class(x)=argmax_y p(y|x)
    </math>
    - říká se jim diskriminativní, protože rovnou "diskriminují"
    - pro různé vstupy prostě rovnou mají nějaké rozdělení
- generativní modely modelují místo toho
    <math>
    class(x)=argmax_y p(x|y)p(y)
    </math>
    což je totéž, co
    <math>
    class(x)=argmax_y p(x,y)
    </math>
    - říká se jim generativní, protože jakoby můžou generovat něco z té pravděpodobnosti na té vnitřní věci
    - např. HMM můžou generovat text


Jazyková data pro strojové učení
----
- wut
- netušim, co ta otázka znamená

Jazykové modely
---
- n-gram - markov chain
- MLE - maximum likelihood estimate
    - hledáme takové parametry, že viděná data jsou co nejvíc pravděpodobná
    - v praxi - p(w|history1, history2) = c(w,history1,history2)/c(history1,history2)
- moc nevim, co bych sem napsal, abych nepředbíhal

Vyhlazování modelů
---
- smoothing
    - nechceme tam nuly, dělají tam bordel, když máme jiná data než trénovací
    - upravíme pravděpodobnost tak, aby byla nenulová, ale součet pořád 1
- přidávání 1
    - no, to jde, ale sníží nám to pravděpodobnosti moc, protože musíme vydělit velikostí slovníku
- přidávání <1
    - lepší
- good turing
    - p(w) = (c(w)+1) * N(c(w)+1) / (|T| * N(c(w)))
    - N je "metapočet", tj. počet slov
    - musí se ještě normalizovat přes slova
- linear interpolation
    - kombinuje víc n-gramových modelů
    - kombinuje je na základě lambd 
    - najdeme lambdy podle MLE 
        - tj. hledáme takové lambdy, aby se nám maximalizovala pravděpodobnost těch dat
        - prý to znamená, že minimalizujeme cross-entropii
        - maximalizujeme sum_i log(p_l(w_i|h_i)) na heldout datech  
    - EM algoritmus je, jak to děláme
        - uděláme nějaký náhodný nenulový lambdy
        - pro ně spočítáme "expected count"
            - tj. c(l_j) = sum_i (l_j*p(w_i|h_i)/p_l(w_i|h_i))
        - další lamdy
            - l'_j = c(l_j)/sum_k l_k
        - ehh '
        - konverguje to
        - můžeme dělat lambdy na bucketech, což moc nechápu
            
Noisy channel models, decoding
---
- nevim, co sem psát
- jakoby předpokládáme opak
- tj. děláme třeba jazyk -> ocr
- předpokládáme nějaký originál a nějakou scrambling funkci
    - např. při spellcheckingu - je originál, a člověk to nějak zašmodrchá a udělá chybné slovo
        - nakonec ale máme jenom to chybné slovo
        - takže pak musíme udělat pravděpodobnost původního krát pravděpodobnost scramblingu původního na to naše
    - stejně ale děláme OCR
        - tj. předpokládáme původní text, a ten se "scramblil" na OCR
    - píše tu, že tak se dělá i MT
        - tak to si nejsem jistý, jak to myslí
        - měl jsem dojem, že trans. model má stejně nakonec p(target|source)
        - ale je možný fakt, že je to naopak, protože frázový tabulky jsou oboustraný... nevim
    
Parametry modelu, prostor hypotéz
---
- wtf
- nevim co chce básník říci


Teoretické aspekty strojového učení (PAC)
---
- popíšu jenom PAC, na víc nemám čas :(
- PAC - probably approximately correct classifier
- odsud https://www.youtube.com/watch?v=Tw8PmfMXuDg
- mějme:
    - X input space
    - Y output space 0,1
    - C koncept space - rodina klasifikátorů
    - věci se z X tahají podle pravděpodobnosti P_X - iid
    - existuje skutečný klasifikátor a je z koncept space C, C^*
    - err(C_n) = P_X(C_n delta C^*)
    - delta je (C_n - C^*) sloučení (C^* - C_n)
    - tj. error je pravděpodobnost chyby (předpoklad: tréning a test mají stejnou pravděpodobnost)
- potom algoritmus je PAC, když
    - pro každé delta, epsilon >0 existuje N<infty, že
        - Prob(err(C_n)>epsilon)<delta
        - pro všechna n>N
        - a to vše pro každou distribuci a každý zlatý klasifikátor
- jestli to chápu dobře, tak pak C je PAC-learnable, když takový algoritmus existuje pro danou množinu konceptů C
    - (píšou "třídu", ale podle mě je možné říct "množina")
- na wiki to nějak spojují s jakousi VC teorií, o té jsem nikdy neslyšel a nemám na to čas :(
