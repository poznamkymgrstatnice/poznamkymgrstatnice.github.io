---
layout: post
title: Datové struktury
---

binární stromy
---
- máme lineárně uspořádané univerzum U, v něm podmnožinu S
- podmnožinu S chceme reprezentovat
- BVS je úplný binární strom (tj. každý vrchol je buď list nebo má 2 syny) , kde existuje bijekce mezi nody a věcmi z S, vlevo menší vpravo větší
- vnitřní nody reprezentují věci, listy reprezentují intervaly
    - podle Koubka to "vyhlíží logičtěji" s nima
    - supr
    - normálně se ty listy můžou vynechávat
- vyhledávání trvá podle hloubky - musíme vyvažovat

- jednoduchá rotace: 
    - přehodíme dva vnitřní nody a převěsíme syny
- dvojitá rotace:
    - popřevěšujeme tři nody
    - v w u , v je syn w, u je syn v na "druhou stranu"
        - pověsíme nahoru u, jako jeho syny v a w, převěsíme
- AVL stromy
    - pro každý nod se hloubka L a P podstromu liší max o 1
        - v každém nodu si pamatuujeme "vyváženost", tj. který podstrom je jak velký
        - nemusíme si pak pamatovat samotné hloubky
    - hloubka kořene je Phi(log n), tj. shora i sdola omezený
    - po vložení musíme dělat kontrolu
        - jedeme od vloženého směrem nahoru
        - u prvního nevyrovnaného uděláme rotaci
        - pokud je to left-left, uděláme jednoduchou rotaci
        - pokud je to left-right, uděláme dvojitou
        - u první kontroly končíme (!!!)
    - mažeme tak, že nahradíme nod nejpravějším levého substromu a jeho subtree pak hodíme místo něho
        - kontrola musí jet až nahoru, protože u rotace jsme snížili, takže musíme jet až nahoru
- červenočerné stromy
    - listy jsou černé
    - když je červený, je buď kořen, nebo je jeho otec černý 
    - stejný počet černých od kořene ke všem listům
        - takže hloubka je O(log n)

    - vyvažování:
        - právě vložený je vždycky červený
        - po vložení řešíme:
            - když je otec černý nebo otec kořen, trivia
            - když je otec červený a strýc červený, přebarvíme otcovskou generaci, ale kvůli počtům musíme zčervenit děda a jet výš
            - pokud tohle udělat nemůžeme, rotujeme
        - při mazání:
            - zase je to komplikovanější než insert, kdo by to čekal
            - kašlu na to
    - liší se to od AVL proto, že i u delete musíme provést jenom max. dvě rotace

haldy
---
- halda nepotřebuje efektivní operaci member
- chceme ale efektivní MIN, a delete/deletemin
    - chceme taky DELETE / INC / DEC, ale dostaneme k tomu umístění prvku, takže nemusíme hledat
    - hledat musíme umět minimum

- halda je na to

- halda je libovolný (i nebin) strom, kde otec je menší než synové

- reg halda
    - halda, kde každý vrchol má max. N synů (u binhaldy 2); 
    - nody jsou očíslované, že
        - když vrchol není list, každý menší má právě N synů, každý větší je list
        - tj. plní jakoby pole zleva
    - výška je log_N n
    - bublání a tak

    - makeheap: vytvoříme haldu, naplníme, u každého bubláme dolu
        - je to nějaký jednodušší nebo co
    - heapsort: opakovaně mažeme min z haldy

- leftist halda
    - nalevo nakloněná halda
    - je to halda
    - npl(x) - null path length - je největší perfektní bintree co je v podstromu pod x (a je rooted v x)
        - npl(levo) >= npl(pravo), tj vlevo vždycky "těžší"
    - The right path (path from root to rightmost leaf) is as short as any in the tree.
        - jo to platí
    - výhoda: pravé podstromy jsou velmi krátké
    - slučujeme:
        - rekurzivně
        - vezmeme tu s menším kořenem, její levý podstrom dáme doleva, jako pravý podstrom sloučení pravého podstromu a té druhé haldy
        - počet rekurzivních volání je součet pravých cest, proto se nám hodí krátké pravé cesty
        - i tak je nejhorší složitost merge O(log)

- fibonacciho halda
    - fibonacciho halda je taková divná věc, co ani neni strom (!!)
    - je to víc stromů, kde si navíc držíme ukazatel na nejmenší
    - každý z těch stromů je klasická halda, tj otec menší synů
    - nerozpadlá vypadá tak, že každá z nich má izomorfnost s binomiálním stromem a nejsou dvě stejně
        - ale my jí povolíme se "dočasně" rozpadnout, že to pak později konzolidujeme
    - některé uzly jsou označeny
    - insert jenom vloží nový strom kde je jenom ten jeden uzel
    - delete min nechá rozpadnout min a pak spojuje
    - amortizovaná složitost (??) tohohle delmin je O(rank(H)) kde rank je počet stromů, a to je <= log (n)
    - při decrease, co by rušila haldovitost - syna prostě odebereme a hodíme jako další strom
        - otcové, co jsme jim usekli jednoho syna, obarvíme
        - pokud jim chceme sekat i druhého syna, tak je radši taky usekneme a hodíme jako další strom
        - a rekurzivně nahoru
    - z nějakého důvodu to funguje
        - ale to rank <= log(n) funguje jenom kvůli tomu rozpadávání
    - Proto se hodně používají v grafových algoritmech, 
kde umožňují v mnoha případech dosáhnout asymptoticky téměř lineární 
složitosti. Neznáme však žádné  
expe\-rimentální výsledky, které by porovnávaly použití Fibonacciho 
hald a např. $d$-regulárních hald v těchto grafových 
algoritmech v praxi. Takže neznáme podmínky, za 
kterých jsou Fibonacciho 
haldy lepší než třeba $d$-regulární haldy, 
ani nevíme, do jaké míry je to jen teoretický 
výsledek a do jaké míry jsou opravdu prakticky použitelné. 

trie
---
- prostě prefixový strom, kde nody jsou prefixy a listy jsou slova
- na texty se to hodí - jde rychle dělat reTRIEvaly, delety, insery, sorty, a tak.
- koubek k tomu v prvním díle datovek nemá nic, druhého dílu se bojím, protože jsem si ho neupravoval

B stromy
---
- koubek tu má definovaný (a,b) stromy, B-stromy jsou pak jejich omezení, 23 stromy jsou pak jejich ještě větší omezení
- takže následují ab stromy, pak pudu zase zpátky k B stromům
- dáme a,b, že 2<=a<=(b+1)/2
- potom
    - a,b strom je takový, že každý nod má mezi a-b dětských nodů
    - root má maximálně b synů a minimálně 2
    - cesty z rootu do listu jsou všechny stejně dlouhé
- výška stromu s n listy je O(log n) maximálně
- pokud necháme v listech to, co chceme reprezentovat, a ve vnitřních nodech si budeme pamatovat největší spodní prvek, tak pak najednou nemusíme mít nic v listech, protože všechno je v těch intervalových nodech, kromě nejpravějšího
    - ale to je takový nepřehledný podle K
        - naopak většina literatury používá notaci, co Koubkovi přišla nepřehledná, tak já nevim, používám ji i já dál
- B strom je totéž, jenom vezmeme n a horní zaokrouhlení n/2, a 2,3 je jeho případ, následující na něm platí úplně stejně
- používám nekoubkovskou notaci, viz http://web.cse.ohio-state.edu/~gurari/course/cis680/cis680Ch13.html
- insert
    - vložíme do listu
    - jestli je přeplněný, pošleme prostřední do rodiče 
- když mažem, vezmeme si místo něj buď nejlevějšího nebo nejpravějšího možného
    - mělo by to jít, protože min 2 syni
    - když se tím stane, že tam nic neni
        - když to jde, vezmeme si plného souseda a hodíme ho do parenta , toho si vememe
        - když to nejde - se sousedem ho sloučíme, resp. souseda dáme do otce a pověsíme se za něj

hešování - řešení kolizí
---
- obecně: v hešování pole reprezentujeme menším polem
    - což funguje dobře, dokud nemáme kolize v mapovací fci
    - možná řešení následují jako podbody
    - m přihrádek, n prvků, α=n/m
- separované řetězce
    - prostě se to dá za sebe do řetězce
    - očekávaná délka řetězců
        - m prihradek, n prvku, rovnomerne rozdeleni -> α
    - rozptyl
        - α(1-α) , "standardní odvození binomiálního rozdělení"
    - pokud α<=1, pak horní odhad očekávané délky maximálního řetězce je O(log n / log log n)
    - pokud α>=0.5, pak je to i dolní odhad 
    - očekávaný počet testů: 
        - neúspěšné: e^(-α)+α; úspěšné: 1+α/2
    - pokud řetězce řadíme, tak neúspěšné má o něco méně
    - nevýhoda: nevyužití alokované paměti, ukazatelé
        - úžasný nápad: strkáme vše do téže tabulky... mě to přijde jako šílenost, ale ve skriptech je to na 500 stránek
- hašování s přemísťováním
    - V tabulce na řádku i buď nic není (= do i se nic nezahešovalo), nebo tam začíná řetězec od něčeho, co koliduje v *h(i)*, nebo je tam prostředek nějakého jiného řetězce (taky jenom tehdy, pokud se do i nic nezahešovalo).
        - Pokud tam při vkládání nic neni, dám to tam.
        - Pokud tam je prostředek jiného řetězce, přemístím ten prostředek (od toho název metody) do jiného volného místa a vkládaný prvek dám do nově uvolněného místa.
        - Pokud tam je začátek správného řetězce, dojedu na konec a dám to za něj.
    - potřebuji tedy next a previous
    - počet testů stejný jako u sep.řet.
- hašování s 2 ukazateli
    - V řádku *i* je úplně jedno, co je v položce *key*, resp. nijak to nesouvisí s výsledkem hashovací funkce - co souvisí je položka *begin*, co říká, kde řetězec k danému číslu začíná
    - nemusíme nic přemísťovat, jenom ukážeme na správný začátek
    - očekávaný počet testů v úspěšném případě:

<script type="math/tex; mode=display">
1+\frac{\alpha^2}{6}+\frac{\alpha}{2}
</script>
    
    - v neúspěšném případě:

<script type="math/tex; mode=display">
1+\frac{\alpha^2}{2}+\alpha+e^{-\alpha}(2+\alpha)-2
</script>
        - naprosto netušim co to znamená nebo jak to interpretovat
- srůstající hešování
    - Máme řetězce jako v minulých dvou algoritmech, ale pokud se trefím doprostřed řetězce, tak ho nepřemísťuji, ale srostu s ním. Protože nic nepřemísťujeme, není potřeba previous.
    - standardní - normální tabulka.
        - LISCH - vkládám na konec řetězce
        - EISCH - vkládám hned za prvek
    - "Nestandardní" – bez písmene navíc :) – tabulka rozšířena o pomocnou tabulku, kam ukládám jako první
        - LICH - vkládám na konec řetězce
        - EICH - vkládám hned za prvek
        - VICH - komplikovanost, viz dál
    - LISCH, EISCH
        - očekávaný počet testů v neúspěšném případě

<script type="math/tex; mode=display">
\frac{1}{4}(e^{2\alpha}-1-2\alpha)
</script>
        - v úspěšném - LISCH

<script type="math/tex; mode=display">
1+\frac{1}{8\alpha}(e^{2\alpha}-1-2\alpha)+\frac{\alpha}{4}
</script>
        - v úspěšném - EISCH - přibližně

<script type="math/tex; mode=display">
\frac{1}{\alpha}(e^{\alpha}-1)
</script>
    - na ty nestandardní kašlu, to fakt nedávám

Univerzální hešování
--
- máme sadu hašovacích funkcí, z nich vybíráme náhodně při běhu
- definice:
	- U je univerzum klíčů, H je konečná množina hašovacích funkcí, co mapuje U do m={0....m-1}; 
	- H je univerzální, když pro každé dva klíše x,y z U, co se nerovnají:
		- <math>|{h in H|h(x)=h(y)}|=\frac{|H|}{m}</math>
	- tj. když vybereme náhodně h z H, pravděpodobnost kolize mezi x a y je 1/m, pro každé x a y.
- z toho plyne:
	- mějme h náhodně z H. Hašujeme n klíčů do m slotů; pro daný klíč x pak E[#kolizí] < n/m = alpha = faktor naplnění
- konstrukce univerzální množiny
	- nechť m je prvočíslo.
	- dekompozujeme k na r+1 čísel menších než "m" - neboli taky "base m" - kdyby m bylo 2, tak to budou bity, atd.
	- vezměme si náhodné a=<a0, ... ar>, každé z nich je náhodně zvolené z {0...m-1}
	- pak hešovací funkce <math>h_a(k)=(\sum_{i=0}^{r}a_ik_i)mod m</math>
		- tj. skalární součin toho náhodného <math>a</math> a <math>k</math> mod <math>m</math>
- jak velká je tahle naše množina?
	- <math>H=m^{r+1}</math>
- tímto končí MIT přednáška, ale Koubkova skripta tady teprve začínají

perfektní hešování
----
- vim dopředu množinu klíčů velkou n
- chci O(n) tabulku, takovou, že hledání je O(1) v nejhorším případě
- idea
	- 2-level scheme with universal hashing at both levels
	- 2. level nemá žádné kolize
	- neznamená to, že výsledek prvního heše zahešujeme, ale naopak - že v haštable místo řetězce máme ještě jednu haštable - musíme si tam uložit velikost (je různá v každé kolizi) a číslo z univ. hashingu
		- zajímavý.... nevim, co z toho bude
- Proč ve 2. levelu není kolize?
	- If there are <math>n_i</math> items, that hash to level1 slot <math>i</math>, then we are gonna use <math>m_i=n_i^2</math> in level 2 table <math>S_i</math>
		- level2 tabulky jsou velmi řídké, aby se daly dobře najít fce bez kolizí
- level 2 analysis
	- thm: zahešuji n klíčů do m=n^2 slotů používáním náhodné fce h z univerza H => E(#kolizí) < 1/2 - tj. nebudu mít expected ani jednu kolizi
		- důkaz v podstatě z definice univerzálního hešování
	- thm: p(no kolize) > 1/2
		- důkaz použije markovovu nerovnost
		- to je vtipný jak je to easy
- velikost v levelu 1 je m=n, tj. vezmeme tabulku velkou jako vše
- n_i nechť je počet věcí, co se nahešují do slotu i; v levelu 2 tedy použijeme n^2
- <math>E(total storage) = n+E\left(\sum_{i=0}^{n=1}\theta\left(n_i^2\right)\right)</math>
- a teď v MIT je "copt out", protože řekne "to už jsme dělali v bucket sortu", a řekne, že je to shora i sdola omezeno n a tím končí - copt out!
- ale mě to asi stačí

Třídění ve vnitřní a vnější paměti
---
- to je taky megaotázka na několik semestrů... tak popořádku
- selection sort
    - úplně nejintuitivnější třídění
    - najdeme nejmenší prvek a dáme ho doleva
    - O(n^2) pro všechny případy
- bubble sort
    - mě přijde v podstatě úplně stejný jako selection sort
    - je fakt, že se do něj lépe implementuje, že pozná hotovost
    - ale jinak je to fakt n^2
- cocktail sort
    - oboustraný bubble
    - nemá moc žádnou výhodu a je takový k ničemu
- insertion sort
    - všechno nalevo od ukazatele už je seřazené
    - ukazatel začíná vlevo
    - bubláme to, na co ukazuje ukazatel doleva tak dlouho, dokud část vlevo není úplně seřazená
    - je to pořád O(n^2), ale má určité výhody oproti jiným kvadratickým algoritmům - je online, je stable, nechce moc místa
- bucket "sort"
    - rozkouskuje věci do menších bucketů
    - to je víceméně všechno
    - neni to moc třídící algoritmus, spíš nám to "předtřídí" pole, ale nijak výrazně nám to asymptoticky nepomůže
    - average case toho třídění je O(n+k) kde k jsou buckety a n počet
- quicksort
    - pivot, left, right
    - hýbeme levého zleva doprava a naopak
        - ani jeden nesmí překročit pivota
    - average je O(n log n)
    - nejhorší je ale pořád O(n^2)
    - není stable 
- merge
    - rekurzivně dělíme, řadíme a mergujeme
    - narozdíl od quicksortu nezálneží na tom, co dostane za data, chová se furt stejně
    - i nejhorší případ O(n log n)
    - ale narozdíl od QS má víc rekurzí, tj. víc paměti s obsluhováním rekurze
- "tree sort"
    - uděláme BVS, uděláme traversal
    - that is basically it
    - traversal trvá maximálně O(n), to je ízy
    - pokud to je ale blbý strom, tak je to O(n^2) a to je na prd 
    - balancing - no to už jsem psal na začátku, ale radši znova
        - AVL (ostatní chybí)
        - balance factor = rozdíl výšky levého a pravého syna (u listů 0) 
        - balance mezi 1 a -1 je ok, víc už neni ok
        - pak je tree sort O(n log n), to je supr
- heap sort
    - zase O(n log n)
- radix sort
    - řadíme tak, že si to napíšeme jako čísla a řadíme každou pozici extra
    - buďto LSD - řadíme zprava a přehazujeme tam a zpátky
    - nebo MSD - řadíme zleva a do bucketů
    - celé O(kn), kde k je délka řetězce (neměli bysme ho omezovat)

Dolní odhady pro uspořádání (rozhodovací stromy)
----
- decision tree má vždycky n! listů
- a nějak se z výšky dá odvodit, že výška je minimálně n log n
- eh.... tohle je nějak jednoduchá otázka

Relaxované vyhledávací stromy
---
- NE.
